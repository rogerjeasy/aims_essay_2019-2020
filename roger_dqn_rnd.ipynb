{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBuffer:                                         ## Experience replay\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32) #Initialization of the set of all current states\n",
    "        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32) # Initialization of the set of all next states\n",
    "        self.acts_buf = np.zeros(size, dtype=np.int32)                # Initialization of the set of all actions\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)     # Initialization of the set of all rewards\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr = 0                                         # Initial numbers\n",
    "        self.size = 0\n",
    "        self.max_size = size    \n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):  # Store one transition <s,a,r,s'>\n",
    "        self.obs1_buf[self.ptr] = obs.flatten()           # Return an array to one dimension\n",
    "        self.obs2_buf[self.ptr] = next_obs.flatten()\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size      # Variation of index\n",
    "        self.size = min(self.size+1, self.max_size)  \n",
    "\n",
    "    def sample_batch(self, batch_size=32):                 # Store all transitions as dictionnary.\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        return dict(obs1=self.obs1_buf[idxs],                      # All current states\n",
    "                    obs2=self.obs2_buf[idxs],                       # All next states\n",
    "                    acts=self.acts_buf[idxs],                        # All actions\n",
    "                    rews=self.rews_buf[idxs],                         # All rewards\n",
    "                    done=self.done_buf[idxs])                         # Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateNetwork(torch.nn.Module):                          # Neural network to get the RND \n",
    "    def __init__(self,num_states, hidden_units,num_actions):    # Initialization\n",
    "        super(CreateNetwork, self).__init__()\n",
    "        self.num_states = num_states\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.first_layer = torch.nn.Linear(self.num_states,self.hidden_units, 'linear')\n",
    "        self.third_layer = torch.nn.Linear(hidden_units,hidden_units, 'linear')\n",
    "        self.second_layer = torch.nn.Linear(self.hidden_units, self.num_actions,'linear')\n",
    "        self.last_activa = torch.nn.Softmax(dim=1)       # normalize the tensor along each row.\n",
    "        \n",
    "    def forward(self,input_state):                            # The input is the state\n",
    "        input_state = F.relu(self.first_layer(input_state))\n",
    "        input_state = F.relu(self.third_layer(input_state))\n",
    "        output = F.relu(self.second_layer(input_state))\n",
    "        #output = self.last_activa(input_state)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class RND:                                                  \n",
    "    \n",
    "    def __init__(self, hidden_units):\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_actions = env.action_space.n\n",
    "        \n",
    "        # Target network\n",
    "        self.target_network = CreateNetwork(env.observation_space.shape[0], hidden_units,env.action_space.n) # target network\n",
    "        \n",
    "        # Predictor network\n",
    "        self.predictor_network = CreateNetwork(env.observation_space.shape[0], hidden_units,env.action_space.n) # predictor network\n",
    "        self.optimizer = torch.optim.Adam(self.predictor_network.parameters(),lr=0.001)\n",
    "        \n",
    "        \n",
    "    def Intrinsic(self,inputs):                                 # The reward function\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        target_output = self.target_network(inputs).detach()\n",
    "        predictor_output = self.predictor_network(inputs)\n",
    "        int_reward = torch.pow(target_output - predictor_output,2).sum() # The L2 norm\n",
    "        return int_reward                                                      # return the intrinsic reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "    \"\"\"A simple mlp to be used for approximating Q-functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_states, hidden_units, hidden_activations, num_actions):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.InputLayer(input_shape=(num_states,))\n",
    "        self.hidden_layers = []\n",
    "        for i, n in enumerate(hidden_units):\n",
    "            self.hidden_layers.append(tf.keras.layers.Dense(\n",
    "              units = n, \n",
    "                activation = hidden_activations[i], \n",
    "              kernel_initializer = 'RandomNormal'))\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units = num_actions, \n",
    "            activation = 'linear', \n",
    "            kernel_initializer = 'RandomNormal')\n",
    "      \n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        z = self.input_layer(inputs)\n",
    "        for layer in self.hidden_layers:\n",
    "            z = layer(z)\n",
    "        output = self.output_layer(z)\n",
    "        return output\n",
    "    \n",
    "class DQN:\n",
    "\n",
    "    def train(Q_main, Q_targ, gamma, buffer, batch_size, optimizer, env):\n",
    "        batch = buffer.sample_batch(batch_size=32)  # Sample of the store transitons\n",
    "        o = batch['obs1']\n",
    "        a = batch['acts']\n",
    "        r = batch['rews']\n",
    "        o2 = batch['obs2']\n",
    "        d = batch['done']\n",
    "        \n",
    "        # number of actions for one-hot encoding\n",
    "        num_acts = env.action_space.n\n",
    "        rewardd = RND(10)\n",
    "        # Create target for bellman update    Y_i\n",
    "        gamma = 0.94\n",
    "        target_action_values = np.max(Q_targ.predict(o2), axis=1)\n",
    "        target_action_values = np.where(d, r, r+gamma*target_action_values)\n",
    "        \n",
    "        \n",
    "        # Find the MSE loss between the taget and actual Q-values for the batch \n",
    "        with tf.GradientTape() as tape:\n",
    "            main_action_values = Q_main(np.atleast_2d(o.astype('float32'))) #we want to Convert inputs to arrays\n",
    "                                                                            # with at least two dimension.\n",
    "            selected_action_values = tf.math.reduce_sum(main_action_values * tf.one_hot(a, num_acts), axis=1)\n",
    "            # selected_action_values = tf.math.reduce_sum(main_action_values * a, axis=1)\n",
    "            loss = tf.math.reduce_sum(tf.square(target_action_values - selected_action_values))\n",
    "            \n",
    "            \n",
    "        # Extract gradients from the tape and apply using the chosen optimizer\n",
    "        variables = Q_main.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables) # returns a gradient tensor\n",
    "        optimizer.apply_gradients(zip(gradients, variables)) #takes list of (gradient, variable) pairs as\n",
    "                                                             # input (that's why you use zip)\n",
    "            \n",
    "    def sync_weights(Q_main, Q_targ):\n",
    "        \"\"\"Copy the weights from the main Q-network to the \n",
    "            target Q-network.\n",
    "        \"\"\"\n",
    "        #self.Q_targ = Q_targ\n",
    "        #self.Q_main = Q_main\n",
    "        variables1 = Q_targ.trainable_variables\n",
    "        variables2 = Q_main.trainable_variables\n",
    "        \n",
    "        for v, w in zip(variables1, variables2):\n",
    "            v.assign(w.numpy())                     # assign the weights of v2 (Q-main) to v1 (Q-target)\n",
    "\n",
    "            \n",
    "    def run(env, agent_params, training_params):\n",
    "        # Set random seed for reproducability\n",
    "        seed_value = 10\n",
    "        np.random.seed(seed_value) # Generate pseudo-random number i\n",
    "        \n",
    "        env = gym.make('MountainCar-v0')\n",
    "        buffer = MemoryBuffer(env.observation_space.shape[0],env.action_space.n,32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Train agent\n",
    "        all_rewards = [] # will contain the sum of extrinsic and intrinsic rewards\n",
    "        total_steps = 0\n",
    "        \n",
    "        \n",
    "        # MODEL PARAMETERS\n",
    "        hidden_dims = agent_params['hidden_layer_sizes']\n",
    "        hidden_activations = agent_params['hidden_layer_activations']\n",
    "        gamma = agent_params['discount_factor']\n",
    "        lr = agent_params['learning_rate']\n",
    "        epsilon = agent_params['epsilon']\n",
    "        min_epsilon = agent_params['min_epsilon']\n",
    "        epsilon_decay = agent_params['epsilon_decay']\n",
    "        \n",
    "        # TRAINING PARAMETERS\n",
    "        num_episodes = training_params['num_episodes']\n",
    "        max_steps = training_params['max_steps']\n",
    "        min_experiences = training_params['min_experiences']\n",
    "        biffer_size = training_params['buffer_size']\n",
    "        batch_size = training_params['batch_size']\n",
    "        copy_step = training_params['copy_step']\n",
    "        plot = training_params['plot_results']\n",
    "        #seed_value = training_params['seed_value']\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(lr)\n",
    "        \n",
    "        Q_main =MyModel(env.observation_space.shape[0], [64,64], [\"relu\", \"relu\"], env.action_space.n)\n",
    "        \n",
    "        Q_targ = MyModel(env.observation_space.shape[0], [64,64], [\"relu\", \"relu\"], env.action_space.n)\n",
    "        rewardd = RND(10)\n",
    "        extr_rew = []             # will contain just extrinsic rewards\n",
    "        \n",
    "        for ep in range(num_episodes):\n",
    "            o = env.reset()\n",
    "            all_rewards.append(0.0)\n",
    "            epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "            epsilon = round(epsilon,3)\n",
    "            extr_rew.append(0.0)\n",
    "            \n",
    "            # play game\n",
    "            for t in range(max_steps):\n",
    "                \n",
    "                env.render()\n",
    "                # take action \n",
    "                if(np.random.random() < epsilon):\n",
    "                    action = np.random.randint(env.action_space.n)\n",
    "                else:\n",
    "                    action = np.argmax(Q_main.predict(np.atleast_2d(o))[0])\n",
    "                \n",
    "           \n",
    "            # step in environment\n",
    "                o2, r, d, _ = env.step(action)\n",
    "                rewardd_i = rewardd.Intrinsic(torch.from_numpy(o))\n",
    "                rewardd_i = rewardd_i.item()\n",
    "                list1 = list(o2)\n",
    "                if list1[0] == 0.6 and list1[1] >= 0.07:\n",
    "                    reward_ex = 0.5\n",
    "                else:\n",
    "                    reward_ex = r\n",
    "                combine_reward = rewardd_i + reward_ex\n",
    "                #print(d)\n",
    "                all_rewards[-1] += combine_reward\n",
    "                extr_rew[-1] += reward_ex\n",
    "                \n",
    "                # store transition\n",
    "                buffer.store(o,action,combine_reward,o2,d)\n",
    "            \n",
    "            # train (action replay)\n",
    "                if total_steps >= min_experiences:\n",
    "                    DQN.train(Q_main, Q_targ, gamma, buffer, batch_size, optimizer, env)\n",
    "\n",
    "                \n",
    "                if total_steps % copy_step == 0:\n",
    "                    DQN.sync_weights(Q_main, Q_targ)\n",
    "                  \n",
    "            # update observation\n",
    "                o = o2\n",
    "            \n",
    "            # update step count\n",
    "                total_steps += 1\n",
    "            \n",
    "            #print(\"episode: {}, epsillon; {}, reward: {}\".format(ep,epsilon, np.mean(extr_rew[-1])))\n",
    "                \n",
    "                if list1[0] == 0.6 and list1[1] >= 0.07:\n",
    "                    return \"episode: {}, epsillon; {}, reward: {}\".format(ep,epsilon, np.mean(all_rewards[-1]))\n",
    "    \n",
    "        if plot:\n",
    "        \n",
    "            dqn_rnd_returns = [r if i<20 else np.mean(all_rewards[i-20:i]) for i, r in enumerate(all_rewards)]\n",
    "            fig, ax = plt.subplots(figsize=(14,7))\n",
    "            ax.set_xlabel(\"Number of Episodes\")\n",
    "            ax.set_ylabel(\"Returns\")\n",
    "            ax.plot(dqn_rnd_returns,'-b', label='DQN+RND')\n",
    "            plt.savefig(\"DQN_RND.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "agent_params = {'hidden_layer_sizes':10,'min_epsilon':0.1,'hidden_layer_activations':'relu','discount_factor':0.9,\n",
    "                        'learning_rate':0.001,'epsilon':1.0,'epsilon_decay':0.94}\n",
    "training_params = {'num_episodes':200,'max_steps':600,'min_experiences':50,'buffer_size':20000,\n",
    "                   'batch_size':128,'copy_step':20,'plot_results' : True}\n",
    "rrr = DQN()\n",
    "buffer = MemoryBuffer(env.observation_space.shape[0],env.action_space.n,128)\n",
    "\n",
    "rnd = RND(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rrr.run(agent_params,training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
